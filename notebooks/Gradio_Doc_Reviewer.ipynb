{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aak0tMKqL0od"
      },
      "outputs": [],
      "source": [
        "#@title You will need an OpenAI API key, or access to some other LLM\n",
        "OPENAI_KEY = \"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV4BgiWzzOh-"
      },
      "source": [
        "A  simple notebook for testing out OpenAI's document summarization abilities using gpt-3.5, and a Gradio interface. This allows you do upload a PDF document, extract the text, then try various prompt designs, chunk sizes, and output sizes to see what affects Summary quality.\n",
        "\n",
        "https://colab.research.google.com/drive/1PwyFmopC1D588aHyYE0Htf2cp8IZwuAk?usp=sharing#scrollTo=iV4BgiWzzOh-\n",
        "\n",
        "Be aware, this code has NO error handling, and can probably break fairly easily!\n",
        "\n",
        "Default behavior uses gpt-3.5-turbo for summarizing document chunks, then gpt-3.5-turbo-16k in order to perform the final summarization. This can be modified fairly easily to have, for example, 3.5-16k doing summaries, and then feeding the results into gpt-4 for higher quality results. Assuming one has access to gpt-4...\n",
        "\n",
        "###Disclaimer\n",
        "Unless required by applicable law or agreed to in writing, the code provided in this notebook is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwU2S8vH4Pvv"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio langchain unstructured pdf2image openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu2xE86D3nMH"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.docstore import document\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZJndwKMT4jz"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "def tiktoken_len(text):\n",
        "  tokens = tokenizer.encode(\n",
        "      text,\n",
        "      disallowed_special=()\n",
        "  )\n",
        "  return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNcYTpLCWtRa"
      },
      "outputs": [],
      "source": [
        "#@title All-in-One 1.0\n",
        "\n",
        "summary_template_default = \"\"\"Context:{text}\n",
        "As an experienced legal analyst, review the Context which is part of a long document to answer the question.\n",
        "{question}\n",
        "\n",
        "Create a detailed and comprehensive outline of the Context with as many facts, descriptions, explanations, reasonings, previous case citations and their relationship to this case, as you can.\n",
        "\n",
        "\"\"\"\n",
        "final_template_default = \"\"\"Context:{text}\n",
        "As an experienced legal analyst, use the Context and the question:\n",
        "{question}\n",
        "\n",
        "Compose a case brief using only information from the Context.\n",
        "Working step by step, organize the Context outline into well-structured paragraphs in the following sections:\n",
        "\n",
        "Case: [name of the case, court, year]\n",
        "Questions Presented: The issues the Court must resolve.\n",
        "Facts of the Case: the parties, facts and events leading to this case.\n",
        "Procedural History: [district court case summary, appeals court case summary, how this issue reached this Court]\n",
        "Analysis: subsections[\"Rules\": detailed explanation of how the Court's considers relevant statutes, interpretations, standards, and tests applicable to this case, \"case law\": names of cases reviewed by the Court and analysis of how the Court relates those cases to the Questions Presented, \"Application\": detailed explanation of how the Rules and Case Law help the Court reach its conclusions]\n",
        "Conclusion: the Court's ruling on the Questions Presented.\n",
        "Dissent:  How it disagrees with the holding of this case.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#def summarize_it(question, chunk_s, out_s, doc):\n",
        "def summarize_it(question, chunk_s, out_s, doc, summary_template, final_template, token_ratio):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = chunk_s, #chunk_s, # number of units per chunk\n",
        "      chunk_overlap = 0, # number of units of overlap\n",
        "      length_function = tiktoken_len, #trying to use tokens as chunking unit instead of characters.\n",
        "      separators=['\\n\\n', '\\n', ' ', ''] # our chosen operators for separating\n",
        "      )\n",
        "  texts = text_splitter.split_text(doc)\n",
        "\n",
        "  s_prompt = PromptTemplate(\n",
        "      input_variables=[\"text\", \"question\"],\n",
        "      template=summary_template\n",
        "  )\n",
        "\n",
        "  f_prompt = PromptTemplate(\n",
        "      input_variables=[\"text\", \"question\"],\n",
        "      template=final_template\n",
        "  )\n",
        "\n",
        "#@title Use this for OpenAI latest: gpt-3.5-turbo, bigger context: gpt-3.5-turbo-16k, functions: gpt-3.5-turbo-0613\n",
        "#summarization takes chunk_s worth of tokens and returns at max that amount divided by 3. So using the 16k model we should be able to summarize a ~35k token document.\n",
        "  llms = OpenAI(temperature=0, openai_api_key=OPENAI_KEY, model_name=\"gpt-3.5-turbo\", max_tokens=int(chunk_s // token_ratio))\n",
        "  llmf = OpenAI(temperature=0, openai_api_key=OPENAI_KEY, model_name=\"gpt-3.5-turbo-16k\", max_tokens=out_s)\n",
        "  summarized_texts = \"\"\n",
        "  for text in texts:\n",
        "    summary_prompt = s_prompt.format(text=text, question=question)\n",
        "    summary = llms(summary_prompt)\n",
        "#    summarized_texts.append(summary)\n",
        "    summarized_texts += summary + \"\\n\"\n",
        "#  return summarized_texts\n",
        "  final_prompt = f_prompt.format(text=summarized_texts, question=question)\n",
        "  final_summary = llmf(final_prompt)\n",
        "  summarized_tokens = int(tiktoken_len(summarized_texts))\n",
        "  return final_summary, summarized_texts, summarized_tokens\n",
        "\n",
        "# function call to rerun the Final Summary\n",
        "def resummarize_it(question, out_s, summarized_texts, final_template):\n",
        "  f_prompt = PromptTemplate(\n",
        "      input_variables=[\"text\", \"question\"],\n",
        "      template=final_template\n",
        "  )\n",
        "  llmf = OpenAI(temperature=0, openai_api_key=OPENAI_KEY, model_name=\"gpt-3.5-turbo-16k\", max_tokens=out_s)\n",
        "  final_prompt = f_prompt.format(text=summarized_texts, question=question)\n",
        "  final_summary = llmf(final_prompt)\n",
        "  return final_summary\n",
        "\n",
        "#function that selects PDF, extracts text to list, extracts page_content, counts tokens, sets token ratio.\n",
        "\n",
        "def upload_file(files, token_ratio):\n",
        "  page_content = []\n",
        "  loader = UnstructuredPDFLoader(files.name)\n",
        "  docs_raw = loader.load()\n",
        "  doc_content = docs_raw[0].page_content[:]\n",
        "  docs_tokens = int(tiktoken_len(doc_content))\n",
        "\n",
        "# This line here lets you tune the max token ratio!!!\n",
        "  new_token_ratio = round(docs_tokens / 12500, 1)\n",
        "  if new_token_ratio > token_ratio:\n",
        "    token_ratio = new_token_ratio\n",
        "  else:\n",
        "    token_ratio = 2.5\n",
        "  return doc_content, docs_tokens, token_ratio\n",
        "\n",
        "def set_template(radio_b, summary_working, final_working):\n",
        "  temp_template = None\n",
        "  if radio_b == \"Summarize\":\n",
        "    temp_template = summary_working\n",
        "  elif radio_b == \"Final\":\n",
        "    temp_template = final_working\n",
        "  else:\n",
        "    temp_template = None\n",
        "  return temp_template\n",
        "\n",
        "def custom_prompt(radio_b, custom_prompt, summary_working, final_working):\n",
        "  if radio_b == \"Summarize\":\n",
        "    summary_working = custom_prompt\n",
        "  elif radio_b == \"Final\":\n",
        "    final_working = custom_prompt\n",
        "  else:\n",
        "    custom_prompt = None\n",
        "  return custom_prompt, summary_working, final_working\n",
        "\n",
        "def restore_templates(radio_b):\n",
        "  summary_working = summary_template_default\n",
        "  final_working = final_template_default\n",
        "  if radio_b == \"Summarize\":\n",
        "    return summary_working, summary_working, final_working\n",
        "  if radio_b == \"Final\":\n",
        "    return final_working, summary_working, final_working\n",
        "  else:\n",
        "    return None, summary_working, final_working\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Row():\n",
        "    with gr.Column(scale=2.5):\n",
        "      with gr.Tab(\"Summarizer\"):\n",
        "        doc = gr.Textbox(lines=12, max_lines=15, label=\"Input Document\", show_copy_button=True)\n",
        "        question = gr.Textbox(label=\"Question\")\n",
        "        with gr.Row():\n",
        "          doc_tokens = gr.Textbox(label=\"Tokens\", scale=1)\n",
        "          chunk_s = gr.Slider(400, 2800, step=100, label=\"Input Chunk Size\", scale=4)\n",
        "          out_s = gr.Slider(500, 4000, step=250, label=\"Output Summary Size\", scale=4)\n",
        "\n",
        "# Document upload thingy\n",
        "\n",
        "        with gr.Row():\n",
        "          token_ratio = gr.State(2.5)\n",
        "          upl_btn = gr.UploadButton(\"Upload\", file_types=[\".pdf\"], file_count=\"single\", size=\"sm\")\n",
        "          upl_btn.upload(fn=upload_file, inputs=[upl_btn, token_ratio], outputs=[doc, doc_tokens, token_ratio])\n",
        "          sub_btn = gr.Button(\"Submit\", variant=\"primary\", size=\"sm\")\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            Long Document Summarizer using gpt-3.5-turbo. Click \"Upload\" to upload a PDF document. The plain text will be extracted and displayed in the \"Input Document\"\n",
        "            window, and Tokens will be calculated. Adjust the Input Chunk Size and Output Summary Size sliders to modify how the chain query\n",
        "            is performed. You can modify the Prompt templates and get additional debugging data from the other tabs.<p>\n",
        "            \"Tokens\" is the token count of the uploaded file. When you upload a file, a token ratio is calculated as Tokens / 12,500.\n",
        "            The instruction for Summarizing will uses an Input/Output token ratio of 2.5 (default) or Tokens / 12,500 to ensure that\n",
        "            the total input tokens to the Final prompt is around or under 13,000. Summarize prompts are performed by gpt-3.5-turbo. Input Chunk Sizes above\n",
        "            2500 can exceed the allowable context limit depending on your prompts. Final prompts are performed by gpt-3.5-turbo-16k. Depending on total input\n",
        "            tokens, Output Summary Size around 3000 tokens should be totally fine, although most of the time, the output is under 1000 tokens.<p>\n",
        "\n",
        "            Given this equation, the Summarizer can process near-arbitrary sized documents into approximately 12,000 input tokens, then convert that to 1-2k output\n",
        "            summaries. Think about those loss-ratios when devising your prompts and expectations around results.\n",
        "            \"\"\")\n",
        "\n",
        "# State Variables for managing things\n",
        "\n",
        "      with gr.Tab(\"Query Customizer\"):\n",
        "        summary_save = gr.State(summary_template_default)\n",
        "        final_save = gr.State(final_template_default)\n",
        "\n",
        "# Basic Instructions for things\n",
        "        query_box = gr.Textbox(lines=12,interactive=True, label=\"Custom Queries, made to order by YOU!\", show_copy_button=True)\n",
        "        with gr.Row():\n",
        "          t_choose = gr.Radio([\"Summarize\", \"Final\"], scale=3, label=\"Try these Fresh Prompts!\")\n",
        "          t_choose.change(fn=set_template, inputs=[t_choose, summary_save, final_save], outputs=query_box)\n",
        "\n",
        "#put some stuff in to save new values to \"summary_template\" and \"final_template\"\n",
        "        with gr.Row():\n",
        "          save_btn = gr.Button(\"Save\", variant=\"primary\", size=\"sm\")\n",
        "          save_btn.click(fn=custom_prompt, inputs=[t_choose, query_box, summary_save, final_save], outputs=[query_box, summary_save, final_save])\n",
        "          clr2_btn = gr.Button(\"Restore Defaults\", variant=\"stop\", size=\"sm\")\n",
        "          clr2_btn.click(fn=restore_templates, inputs=[t_choose], outputs =[query_box, summary_save, final_save])\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            \"Summarize\" is the prompt used for each Chunk of the document.<p>\n",
        "            \"Final\" is used to create the Output Summary.<p>\n",
        "            Your edited queries *MUST* contain the {text} and {question} variables or they won't work.<p>\n",
        "\n",
        "            Click on the Radio buttons to view the default Prompts. Both Prompt templates pass in two variables, {text}, and {question}.\n",
        "            The {text} variable is the passed in document chunk, and {question} is the user-provided Question text from the 'Summarizer' tab.<p>\n",
        "            You can modify the Prompts in this window and click \"Save\" to update the Prompt corresponding to the Radio Button selection, and that Prompt will be used when\n",
        "            you click \"Submit\" from the 'Summarizer'' tab. Click \"Restore Defaults\"\n",
        "            to revert back to the original default prompts. Keep in mind context limits and input chunk sizes as you modify these prompts.\n",
        "            \"\"\")\n",
        "      with gr.Tab(\"Debugging\"):\n",
        "        with gr.Row():\n",
        "          summaries_tokens = gr.Textbox(scale=1, label=\"Summary Tokens\")\n",
        "        chunk_summaries = gr.Textbox(lines=15, label=\"Concatenated text of Intermediate summaries\", show_copy_button=True)\n",
        "\n",
        "# Output Query looks bad. Retry the final summary\n",
        "\n",
        "        re_summarize = gr.Button(\"Rerun Final Summary\", variant=\"primary\")\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            Space for showing some intermediary stuff about how the Query process went. First is \"Summarized Chunks\", which is the individual chunk summarizations for\n",
        "            evaluating how the Summarize query is performing, and also to see how much input data went into the Final query.<p>\n",
        "            \"Rerun Final Summary\" will allow you to take the existing Summarized Chunks and redo just the Final prompt. You need to adjust the question and prompt template\n",
        "            from the other tabs for now.\n",
        "            \"\"\")\n",
        "    with gr.Column(scale=1):\n",
        "      output = gr.Textbox(lines=15, label=\"Output Box\", show_copy_button=True)\n",
        "      sub_btn.click(fn=summarize_it, inputs=[question, chunk_s, out_s, doc, summary_save, final_save, token_ratio], outputs=[output, chunk_summaries, summaries_tokens])\n",
        "      re_summarize.click(fn=resummarize_it, inputs=[question, out_s, chunk_summaries, final_save], outputs=output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue().launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
